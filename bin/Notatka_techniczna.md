### ğŸ“ Notatka techniczna â€” stabilizacja pipelineâ€™u SI (Mistral / Ollama)

**Zakres prac:** luty 2026
**Cel:** zapewnienie ciÄ…gÅ‚oÅ›ci odpowiedzi na czacie firmowym mimo limitÃ³w API i timeoutÃ³w modeli zewnÄ™trznych

---

#### 1. Instalacja i uruchomienie Ollamy (lokalny fallback)

* Zainstalowano OllamÄ™ jako lokalny backend LLM (CPU).
* Skonfigurowano endpoint `/api/chat` z obsÅ‚ugÄ…:

  * `keep_alive` (utrzymanie modelu w RAM),
  * retry przy `ReadTimeout`,
  * dynamiczny `total_timeout` (do 600s przy cold start).
* Wybrany model: `llama3.1:8b` (roboczo, moÅ¼liwoÅ›Ä‡ zmiany na mniejszy).

---

#### 2. Aktualizacja wrappera (`MistralChatManager`)

* Przebudowano metodÄ™ `_post()`:

  * dodano parametr `mistral: bool = True`,
  * **rozÅ‚Ä…czono Å›cieÅ¼ki**:

    * `mistral=True` â†’ wyÅ‚Ä…cznie SDK Mistrala
    * `mistral=False` â†’ wyÅ‚Ä…cznie Ollama
* UsuniÄ™to fallback Ollamy z `except` Mistrala (Plan A).
* Zachowano peÅ‚nÄ… kompatybilnoÅ›Ä‡ kontraktÃ³w z poprzedniÄ… wersjÄ… wrappera.
* KaÅ¼da metoda publiczna (`text_response`, `categorize_response`, `continue_conversation`, itd.) obsÅ‚uguje teraz jawnie `mistral=True/False`.

**Plan A (nieudany):**

* Automatyczny fallback Mistral â†’ Ollama w `except`.
* Problem: blokowanie pipelineâ€™u przez dÅ‚ugie timeouty Ollamy (cold start).
* Decyzja: **porzucenie fallbacku synchronicznego** na rzecz sterowania Å›cieÅ¼kÄ… wyÅ¼ej (daemon).

---

#### 3. Aktualizacja pipelineâ€™u daemona (routing i sterowanie)

* Routing bota (Gerina / Pionier / Aifa) wykonywany **na pierwszym kontakcie**.
* Wprowadzono flagÄ™ stanu:

  * `acive_bot_valided = True/False`
* JeÅ›li selektor (Mistral) **nie potwierdzi bota**:

  * domyÅ›lnie ustawiana jest **Aifa**,
  * dalsza odpowiedÅº idzie **wyÅ‚Ä…cznie przez OllamÄ™**,
  * generacja odbywa siÄ™ **w tle (thread)**.

---

#### 4. Przetwarzanie w tle (Aifa + Ollama)

* Przy `acive_bot_valided=False`:

  * historia rozmowy jest **ucinana do ogona** (`hist_aifa[-12:]`),
  * uruchamiany jest wÄ…tek (`threading.Thread`, `daemon=True`),
  * odpowiedÅº Aify zapisywana jest asynchronicznie przez `save_chat_message`,
  * pipeline gÅ‚Ã³wny **nie jest blokowany**.
* Ograniczono rÃ³wnolegÅ‚oÅ›Ä‡ (semafor) w celu ochrony CPU.

---

#### 5. Komunikacja z uÅ¼ytkownikiem â€” `[warning]`

* Dodano mechanizm komunikatu ostrzegawczego:

  * `[warning]` wstrzykiwany na indeks `-2`,
  * `-1` zawsze zawiera ostatnie pytanie uÅ¼ytkownika.
* TreÅ›Ä‡ ostrzeÅ¼enia:

  * informacja o chwilowych utrudnieniach funkcji SI,
  * brak technikaliÃ³w (modele, tokeny, API),
  * przeprosiny + deklaracja trwajÄ…cych prac.
* Efekt: uÅ¼ytkownik **widzi odpowiedÅº**, nawet jeÅ›li przyszÅ‚a z opÃ³Åºnieniem, i rozumie sytuacjÄ™.

---

#### 6. Testy i weryfikacja

Przetestowano scenariusze:

* brak tokenÃ³w Mistrala (429),
* routing niepotwierdzony,
* peÅ‚na obsÅ‚uga przez OllamÄ™,
* odpowiedzi synchroniczne i asynchroniczne,
* wielokrotne wiadomoÅ›ci uÅ¼ytkownika bez odpowiedzi poÅ›redniej.

**PrzykÅ‚adowy pomiar czasu:**

* pierwsza wiadomoÅ›Ä‡: `18:00:54 GMT`
* odpowiedÅº Aify: `18:09:11 GMT`
* **czas odpowiedzi:** ~8 min 17 s
* **timeout systemowy:** 12 minut
  â¡ odpowiedÅº dostarczona poprawnie, bez zerwania sesji.

---

#### 7. Stan koÅ„cowy

* System dziaÅ‚a w trybie **degradacji kontrolowanej**.
* Brak tokenÃ³w lub awaria Mistrala **nie blokuje czatu**.
* Zawsze pojawia siÄ™ odpowiedÅº (Aifa).
* Architektura odporna na:

  * rate limit,
  * timeouty,
  * cold start modeli,
  * brak dostÄ™pnoÅ›ci agentÃ³w specjalistycznych.

---

Jasne â€” zrÃ³bmy â€Å›ciÄ…gÄ™â€ z krÃ³tkimi fragmentami kodu, Å¼ebyÅ› po miesiÄ…cu nie musiaÅ‚ wertowaÄ‡ caÅ‚ego daemona/pipelineâ€™u. PoniÅ¼sze przykÅ‚ady sÄ… spÃ³jne z aktualnym wrapperem (`mistral: bool = True` w kaÅ¼dej metodzie) i z rozdzieleniem Å›cieÅ¼ek Mistral/Ollama. 

1. Wrapper: jedna metoda `_post`, dwie Å›cieÅ¼ki (Mistral vs Ollama)

```python
def _post(..., mistral: bool = True) -> Optional[str]:
    # OLLAMA
    if not mistral:
        txt = _ollama_chat(...)
        return self._normalize_content_to_text(txt)

    # MISTRAL
    with Mistral(...) as mistral_client:
        res = mistral_client.chat.complete(...)
    return self._normalize_content_to_text(res.choices[0].message.content)
```

2. UÅ¼ycie w kodzie: jawny wybÃ³r backendu per wywoÅ‚anie

```python
# routing / selektor bota (zwykle Mistral)
bot_ident = mgr.categorize_response(prompti, witch_bot_list, max_tokens=100, mistral=True)

# generacja odpowiedzi â€œtanio / lokalnieâ€
ans_local = mgr.continue_conversation_with_system(hist, sys_prmt_aifa, max_tokens=800, mistral=False)

# generacja odpowiedzi â€œnormalnie / chmuraâ€
ans_cloud = mgr.continue_conversation_with_system(hist, sys_prmt_aifa, max_tokens=800, mistral=True)
```

3. Daemon: selekcja bota + flaga `acive_bot_valided`

```python
acive_bot_valided = False
bot_rotation = "aifa"  # default

if latest_user_message_author not in ["gerina", "pionier"]:
    bot_ident = mgr.categorize_response(prompti, witch_bot_list, max_tokens=100, mistral=True)

    bot_ident_norm = (bot_ident or "").strip().lower()
    allowed = {b.strip().lower() for b in (witch_bot_list or [])}

    if bot_ident_norm in allowed:
        acive_bot_valided = True
        bot_rotation = bot_ident_norm
    else:
        acive_bot_valided = False
        bot_rotation = "aifa"
```

4. â€Plan Aâ€ (stary): fallback z `except` â€” odradzamy (blokowaÅ‚ pipeline)
   Tylko jako przypominajka, co porzuciliÅ›my:

```python
try:
    ans = mistral_call()
except Exception:
    # Plan A: tu odpalano ollamÄ™ synchronicznie -> timeouty blokowaÅ‚y pipeline
    ans = ollama_call()
```

5. â€Plan Bâ€ (obecny): gdy routing niepewny â†’ Aifa na Ollamie w tle (thread)

```python
import threading
_OLLAMA_BG_SEM = threading.Semaphore(2)

def _bg_aifa_job(idx: int, hist_snapshot: list, sys_prompt: str):
    try:
        with _OLLAMA_BG_SEM:
            ans = mgr.continue_conversation_with_system(
                hist_snapshot, sys_prompt, max_tokens=800, mistral=False
            )
            if ans:
                save_chat_message("aifa", ans, 0)
    except Exception as e:
        print(f"[BG AIFA ERROR] idx={idx} err={repr(e)}")
```

6. Ogon historii: nie spamujemy kontekstem

```python
# tylko ogon rozmowy
hist_snapshot = list(hist_aifa[-12:])
```

7. `[warning]` na indeksie -2, a pytanie usera na -1

```python
extra_warning = (
    "[warning]\n"
    "Uwaga: mogÄ… wystÄ™powaÄ‡ chwilowe utrudnienia w dziaÅ‚aniu funkcji opartych o SI. "
    "CzÄ™Å›Ä‡ agentÃ³w moÅ¼e byÄ‡ tymczasowo niedostÄ™pna lub dziaÅ‚aÄ‡ w ograniczonym zakresie. "
    "Prace nad usuniÄ™ciem problemÃ³w sÄ… w toku. Przepraszamy za utrudnienia.\n"
)

# zakÅ‚adamy, Å¼e hist_snapshot[-1] to ostatnia wiadomoÅ›Ä‡ usera
if hist_snapshot and hist_snapshot[-1].get("role") == "user":
    hist_snapshot.insert(-1, {"role": "user", "content": extra_warning})
else:
    hist_snapshot.append({"role": "user", "content": extra_warning})

# sanity check
print("âš ï¸ warning index OK:", len(hist_snapshot) - 2)
```

8. Start wÄ…tku tylko gdy `acive_bot_valided=False`

```python
if not acive_bot_valided:
    print(f"ğŸ§µ AIFA BG | start task #{i} | routing_valid={acive_bot_valided}")

    t = threading.Thread(
        target=_bg_aifa_job,
        args=(i, hist_snapshot, sys_prmt_aifa),
        daemon=True
    )
    t.start()
else:
    ans = mgr.continue_conversation_with_system(hist_aifa, sys_prmt_aifa, max_tokens=800, mistral=True)
    if ans:
        save_chat_message("aifa", ans, 0)
```

9. Logi, ktÃ³re realnie pomagajÄ… (Twoje emoji-formaty)

```python
print(f"ğŸ§­ bot={bot_rotation} | validated={acive_bot_valided}")
print(f"ğŸ§  hist_aifa[0]: {hist_aifa[0] if hist_aifa else ''}")
print(f"ğŸ“š hist_aifa.len: {len(hist_aifa)}")
print(f"ğŸ¤– aifa.tail:\n{hist_aifa[-2:]}")
print(f"ğŸ£ catching_gerina: {catching_gerina} | ğŸ” validated: {acive_bot_valided}")
```

10. Pomiar czasu / checkpoint (Å¼eby pamiÄ™taÄ‡ skÄ…d te 12 minut)

```python
start_ts = time.time()

# ... pipeline ...

elapsed = time.time() - start_ts
print(f"ğŸ•’ checkpoint_15s: elapsed_time={elapsed:.2f}s, potrzebne=15s")
```